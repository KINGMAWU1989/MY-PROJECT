{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fd8b571f",
      "metadata": {
        "id": "fd8b571f"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras.datasets import mnist\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "42da0ce2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42da0ce2",
        "outputId": "0cd672ec-5875-4a75-a432-6834da97ccd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n",
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n",
            "uint8\n"
          ]
        }
      ],
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "print(X_train.shape) # (60000, 28, 28)\n",
        "print(X_test.shape) # (10000, 28, 28)a\n",
        "print(X_train[0].dtype) # uint8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3b836729",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "3b836729",
        "outputId": "5e596b33-fcb4-43e5-dc76-27d135715631"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "index = 0\n",
        "image = X_train[index].reshape(28,28)\n",
        "# X_train[index]: (784,)\n",
        "# image: (28, 28)\n",
        "plt.imshow(image, 'gray')\n",
        "plt.title('label : {}'.format(y_train[index]))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0cbc082d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cbc082d",
        "outputId": "17e33d05-9422-402d-a155-208de225ca12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "0.0\n",
            "(60000,)\n",
            "(60000, 10)\n",
            "float64\n",
            "(10000, 10)\n"
          ]
        }
      ],
      "source": [
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "print(X_train.max()) # 1.0\n",
        "print(X_train.min()) # 0.0\n",
        "\n",
        "X_train = X_train[:, np.newaxis, :, :]\n",
        "X_test = X_test[:, np.newaxis, :, :]\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
        "\n",
        "print(y_train.shape) # (60000,)\n",
        "print(y_train_one_hot.shape) # (60000, 10)\n",
        "print(y_train_one_hot.dtype) # float64\n",
        "\n",
        "print(y_test_one_hot.shape) # (60000, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "82ec4fd3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82ec4fd3",
        "outputId": "cc726b72-a2aa-49eb-951e-960be23d9bef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(48000, 1, 28, 28) (48000, 10)\n",
            "(12000, 1, 28, 28) (12000, 10)\n"
          ]
        }
      ],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
        "print(X_train.shape, y_train.shape) # (48000, 784)\n",
        "print(X_val.shape, y_val.shape) # (12000, 784)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0249f043",
      "metadata": {
        "id": "0249f043"
      },
      "outputs": [],
      "source": [
        "def calc_out_shape(N_in, P, F, S):\n",
        "  N_out = ((N_in + 2 * P - F) / S) + 1\n",
        "  return int(N_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "490612cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "490612cb",
        "outputId": "8c0063d9-d18a-4831-caa7-5a90b997f553"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out size: 2\n"
          ]
        }
      ],
      "source": [
        "N_in = 3 #(height, width)\n",
        "P = 0\n",
        "F = 2\n",
        "S = 1\n",
        "\n",
        "print(\"out size:\", calc_out_shape(N_in, P, F, S))\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f6e55143",
      "metadata": {
        "id": "f6e55143"
      },
      "outputs": [],
      "source": [
        "class Conv2d:\n",
        "    def __init__(self, activation, optimizer, filter_num, input_channel, filter_size, stride=1, pad=0):\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "        \n",
        "        self.activation = activation\n",
        "        self.optimizer = optimizer\n",
        "        self.filter_num = filter_num\n",
        "        self.input_channel = input_channel\n",
        "        self.filter_size = filter_size\n",
        "        if self.activation == 'sigmoid':\n",
        "            initializer = XavierInitializer(self.filter_num, self.input_channel, self.filter_size)\n",
        "            self.W = initializer.W(_, _)\n",
        "            self.b = initializer.B(_)            \n",
        "        elif self.activation == 'tanh':\n",
        "            initializer = XavierInitializer(self.filter_num, self.input_channel, self.filter_size)\n",
        "            self.W = initializer.W(_, _)\n",
        "            self.b = initializer.B(_)          \n",
        "        elif self.activation == 'relu':\n",
        "            initializer = HeInitializer(self.filter_num, self.input_channel, self.filter_size)\n",
        "            self.W = initializer.W(_, _)\n",
        "            self.b = initializer.B(_)        \n",
        "        \n",
        "        ## Intermediate data (used during backward)\n",
        "        self.x = None   \n",
        "        self.col = None\n",
        "        self.col_W = None\n",
        "        \n",
        "        ## Gradient of weight / bias parameters\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def _calc_out_shape(self, H, FH, W, FW):\n",
        "        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
        "        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "        return out_h, out_w\n",
        "\n",
        "    def _im2col(self, input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "         ----------\n",
        "         input_data: Input data consisting of a 4-dimensional array of (number of data, channels, height, width)\n",
        "         filter_h: Filter height\n",
        "         filter_w: Filter width\n",
        "         stride: stride\n",
        "         pad: padding\n",
        "         Returns\n",
        "         -------\n",
        "         col: 2D array\n",
        "        \"\"\"\n",
        "        N, C, H, W = input_data.shape\n",
        "        out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "        out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "\n",
        "        img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
        "        col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
        "\n",
        "        for y in range(filter_h):\n",
        "            y_max = y + stride*out_h\n",
        "            for x in range(filter_w):\n",
        "                x_max = x + stride*out_w\n",
        "                col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
        "\n",
        "        col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
        "        \n",
        "        return col\n",
        "\n",
        "    def _col2im(self, col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "         ----------\n",
        "         col:\n",
        "         input_shape: Shape of input data (example: (10, 1, 28, 28))\n",
        "         filter_h:\n",
        "         filter_w\n",
        "         stride\n",
        "         pad\n",
        "         Returns\n",
        "         -------\n",
        "        \"\"\"\n",
        "        N, C, H, W = input_shape\n",
        "        out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "        out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "        col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
        "\n",
        "        img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
        "        for y in range(filter_h):\n",
        "            y_max = y + stride*out_h\n",
        "            for x in range(filter_w):\n",
        "                x_max = x + stride*out_w\n",
        "                img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
        "\n",
        "        return img[:, :, pad:H + pad, pad:W + pad]\n",
        "    \n",
        "    def forward(self, x):\n",
        "        FN, C, FH, FW = self.W.shape ##Filters (FN: number of filters, C: number of channels, FH: filter height, FW: filter width)\n",
        "        N, C, H, W = x.shape ##Input data (N: number of batches, C: number of channels, H: height, W: width)\n",
        "        \n",
        "        ## Output size after 2D convolution (h: height, w: width)\n",
        "        out_h, out_w = self._calc_out_shape(H, FH, W, FW)\n",
        "\n",
        "        ## Expansion of 4D data to 2D by im2col\n",
        "        col = self._im2col(x, FH, FW, self.stride, self.pad)\n",
        "        \n",
        "        col_W = self.W.reshape(FN, -1).T ## Transpose of filter\n",
        "        out = np.dot(col, col_W) + self.b ## Convolution operation\n",
        "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2) ## Output size formatting\n",
        "\n",
        "        self.x = x\n",
        "        self.col = col\n",
        "        self.col_W = col_W\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
        "        \n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        self.dW = np.dot(self.col.T, dout)\n",
        "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "        \n",
        "        dcol = np.dot(dout, self.col_W.T)    \n",
        "        dx = self._col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "        \n",
        "        return dx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "03765fad",
      "metadata": {
        "id": "03765fad"
      },
      "outputs": [],
      "source": [
        "class MaxPool2D:\n",
        "    def __init__(self, pool_h=3, pool_w=3, stride=1, pad=0):\n",
        "        self.pool_h = pool_h\n",
        "        self.pool_w = pool_w\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "        \n",
        "        self.x = None\n",
        "        self.arg_max = None\n",
        "\n",
        "    def _im2col(self, input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "        \"\"\"\n",
        "       Parameters\n",
        "         ----------\n",
        "         input_data: Input data consisting of a 4-dimensional array of (number of data, channels, height, width)\n",
        "         filter_h: Filter height\n",
        "         filter_w: Filter width\n",
        "         stride: stride\n",
        "         pad: padding\n",
        "         Returns\n",
        "         -------\n",
        "         col: 2D array\n",
        "        \"\"\"\n",
        "        N, C, H, W = input_data.shape\n",
        "        out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "        out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "\n",
        "        img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
        "        col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
        "\n",
        "        for y in range(filter_h):\n",
        "            y_max = y + stride*out_h\n",
        "            for x in range(filter_w):\n",
        "                x_max = x + stride*out_w\n",
        "                col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
        "\n",
        "        col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
        "        \n",
        "        return col\n",
        "        \n",
        "    def _col2im(self, col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
        "        \"\"\"\n",
        "       Parameters\n",
        "         ----------\n",
        "         col:\n",
        "         input_shape: Shape of input data (example: (10, 1, 28, 28))\n",
        "         filter_h:\n",
        "         filter_w\n",
        "         stride\n",
        "         pad\n",
        "         Returns\n",
        "         -------\n",
        "        \"\"\"\n",
        "        N, C, H, W = input_shape\n",
        "        out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "        out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "        col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
        "\n",
        "        img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
        "        for y in range(filter_h):\n",
        "            y_max = y + stride*out_h\n",
        "            for x in range(filter_w):\n",
        "                x_max = x + stride*out_w\n",
        "                img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
        "\n",
        "        return img[:, :, pad:H + pad, pad:W + pad]\n",
        "                \n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "        col = self._im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "        arg_max = np.argmax(col, axis=1)\n",
        "        out = np.max(col, axis=1)\n",
        "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.arg_max = arg_max\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout = dout.transpose(0, 2, 3, 1)\n",
        "        \n",
        "        pool_size = self.pool_h * self.pool_w\n",
        "        dmax = np.zeros((dout.size, pool_size))\n",
        "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
        "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
        "        \n",
        "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
        "        dx = self._col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        \n",
        "        return dx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f9460857",
      "metadata": {
        "id": "f9460857"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Convolutional(nn.Module):\n",
        "    def __init__(self, input_channels=3, num_classes=10):\n",
        "        super(Convolutional, self).__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.num_classes = num_classes\n",
        "        self.layer1 = nn.Sequential()\n",
        "        self.layer1.add_module(\"Conv1\", nn.Conv2d(in_channels=input_channels, out_channels=16, kernel_size=3, padding=1))\n",
        "        self.layer1.add_module(\"BN1\", nn.BatchNorm2d(num_features=16, eps=1e-05, momentum=0.1, affine=True,\n",
        "            track_running_stats=True))\n",
        "        self.layer1.add_module(\"Relu1\", nn.ReLU(inplace=False))\n",
        "        self.layer2 = nn.Sequential()\n",
        "        self.layer2.add_module(\"Conv2\", nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1, stride=2))\n",
        "        self.layer2.add_module(\"BN2\", nn.BatchNorm2d(num_features=32, eps=1e-05, momentum=0.1, affine=True,\n",
        "            track_running_stats=True))\n",
        "        self.layer2.add_module(\"Relu2\", nn.ReLU(inplace=False))\n",
        "        self.avg_pool(\"AvgPool1\", nn.AvgPool2D(kernel_size=4, stride=4, padding=0, ceil_mode=False,\n",
        "            count_include_pad=False))\n",
        "        self.fully_connected = nn.Linear(32 * 4 * 4, num_classes)\n",
        "    def forward(self, x):\n",
        "        y = x.clone()\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(-1, 32 * 4 * 4)\n",
        "        x = self.fully_connected(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6798472f",
      "metadata": {
        "id": "6798472f"
      },
      "outputs": [],
      "source": [
        "class Flatten():\n",
        "    def __init__(self):\n",
        "        self.X_shape = None\n",
        "    \n",
        "    def forward(self, X):\n",
        "        ## One-dimensional\n",
        "        X_1d = X.reshape(X.shape[0], -1)\n",
        "        \n",
        "        ## Record of shape\n",
        "        self.X_shape = X.shape\n",
        "        \n",
        "        return X_1d    \n",
        "\n",
        "    def backward(self, X):\n",
        "        ## Return of shape\n",
        "        X = X.reshape(self.X_shape)\n",
        "        \n",
        "        return X\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b83171aa",
      "metadata": {
        "id": "b83171aa"
      },
      "outputs": [],
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "   Iterator to get a mini-batch\n",
        "\n",
        "     Parameters\n",
        "     ----------\n",
        "     X: ndarray, shape (n_samples, n_features) of the following form\n",
        "       Training data\n",
        "     y: ndarray of the following form, shape (n_samples, 1)\n",
        "       Correct answer value\n",
        "     batch_size: int\n",
        "       Batch size\n",
        "     seed: int\n",
        "       NumPy random number seed\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7877a454",
      "metadata": {
        "id": "7877a454"
      },
      "outputs": [],
      "source": [
        "class Scratch2dCNNClassifier:\n",
        "    \"\"\"\n",
        "    Deep neural network classifier\n",
        "\n",
        "     Parameters\n",
        "     --------------\n",
        "     activaiton: {'sigmoid','tanh','relu'}\n",
        "         Types of activation functions\n",
        "     n_nodes: list\n",
        "         Node configuration example [400, 200, 100]\n",
        "     n_output: int\n",
        "         Number of output layers\n",
        "     alpha: float\n",
        "         Learning rate\n",
        "     optimizer: {'sgd','adagrad'}\n",
        "         Types of optimization methods\n",
        "     filter_num: int\n",
        "         Number of filters\n",
        "     filter_size: int\n",
        "         Filter size\n",
        "     stride: int (initial value: 1)\n",
        "         stride\n",
        "     pad: int (initial value: 0)\n",
        "         Padding\n",
        "        \n",
        "    Attributes\n",
        "     -------------\n",
        "     FC [n_layers]: dict\n",
        "         A dictionary that manages the connection layer\n",
        "     activation: dict\n",
        "         A dictionary that manages the activation function\n",
        "     self.epochs: int\n",
        "         Number of epochs (initial value: 10)\n",
        "     self.batch_size: int\n",
        "         Batch size (initial value: 20)\n",
        "     self.n_features: int\n",
        "         Number of features\n",
        "     self.val_is_true: boolean\n",
        "         Presence or absence of verification data\n",
        "     self.loss: empty ndarray\n",
        "         Record losses on training data\n",
        "     self.loss_val: empty ndarray\n",
        "         Record loss on validation data       \n",
        "    \"\"\"    \n",
        "    def __init__(self, activation, n_nodes, n_output, lr, optimizer, filter_num, filter_size):\n",
        "        self.select_activation = activation\n",
        "        self.n_nodes = n_nodes\n",
        "        self.n_output = n_output\n",
        "        self.lr = lr\n",
        "        self.select_optimizer = optimizer\n",
        "        \n",
        "        self.filter_num  = filter_num    ## Number of filters\n",
        "        self.filter_size   = filter_size    ## Filter size        \n",
        "        self.stride          = 1               ##stride\n",
        "        self.pad             = 0               ##Padding\n",
        "            \n",
        "    def _initialize_n_layers(self):\n",
        "        \"\"\"\n",
        "        Initialize the N layer.\n",
        "         When the sigmoid function and tanh function are activation functions: Xavier is the initial value\n",
        "         If the ReLU function is an activation function: He is the initial value\n",
        "        \"\"\"\n",
        "        self.activation = dict()\n",
        "        self.FC = dict()\n",
        "        ##Instantiation of FC class that connects the pooling layer to the fully connected layer\n",
        "        if self.select_activation == 'sigmoid':\n",
        "            self.FC[0] = FC(self.out_size, self.n_nodes[0], \n",
        "                            XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
        "            self.activation[0] = Sigmoid()\n",
        "        elif self.select_activation == 'tanh':\n",
        "            self.FC[0] = FC(self.out_size, self.n_nodes[0], \n",
        "                            XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
        "            self.activation[0] = Tanh()\n",
        "        elif self.select_activation == 'relu':\n",
        "            self.FC[0] = FC(self.out_size, self.n_nodes[0], \n",
        "                            HeInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
        "            self.activation[0] = ReLU()\n",
        "\n",
        "        ##Instantiation of FC class between fully coupled layers\n",
        "        for n_layer in range(len(self.n_nodes)):            \n",
        "            if n_layer == len(self.n_nodes) -1:\n",
        "                if self.select_activation == 'sigmoid':\n",
        "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_output, \n",
        "                                              XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
        "                    self.activation[n_layer + 1] = Softmax()\n",
        "                elif self.select_activation == 'tanh':\n",
        "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_output,\n",
        "                                              XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
        "                    self.activation[n_layer + 1] = Softmax()\n",
        "                elif self.select_activation == 'relu':\n",
        "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_output,\n",
        "                                              HeInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
        "                    self.activation[n_layer + 1] = Softmax()\n",
        "            else:\n",
        "                if self.select_activation == 'sigmoid':\n",
        "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_nodes[n_layer+1], \n",
        "                                              XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
        "                    self.activation[n_layer + 1] = Sigmoid()\n",
        "                elif self.select_activation == 'tanh':\n",
        "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_nodes[n_layer+1],\n",
        "                                              XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
        "                    self.activation[n_layer + 1] = Tanh()\n",
        "                elif self.select_activation == 'relu':\n",
        "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_nodes[n_layer+1],\n",
        "                                              HeInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
        "                    self.activation[n_layer + 1] = ReLU()\n",
        "                    \n",
        "    def _calc_out_shape(self, H, FH, W, FW, layer):\n",
        "        \"\"\"\n",
        "        A function that calculates the output size of the Convolution and Pooling layers\n",
        "        \"\"\"        \n",
        "        if layer == 'conv':\n",
        "            out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
        "            out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
        "        elif layer == 'pool':\n",
        "            out_h = int(1 + (H - FH) / self.stride)\n",
        "            out_w = int(1 + (W - FW) / self.stride)\n",
        "        \n",
        "        return out_h, out_w    \n",
        "    \n",
        "    def fit(self, X, y, epochs=10, batch_size=20):  \n",
        "        self.epochs = epochs                            ## Number of epochs    \n",
        "        self.batch_size = batch_size               ## Batch size\n",
        "        self.loss = np.zeros(self.epochs)        ## For output of learning curve / objective function (training data)\n",
        "        self.loss_val = np.zeros(self.epochs) ## For output of learning curve / objective function (verification data)        \n",
        "        \n",
        "        ##Instantiation of optimization class\n",
        "        if self.select_optimizer == 'sgd':\n",
        "            self.optimizer = SGD(self.lr)\n",
        "        elif self.select_optimizer == 'adagrad':\n",
        "            self.optimizer = AdaGrad(self.lr)            \n",
        "        \n",
        "        ##Input data shape\n",
        "        self.input_channel = X.shape[1]\n",
        "        self.input_h = X.shape[2]\n",
        "        self.input_w = X.shape[3]\n",
        "        \n",
        "        ##Instantiation of convolution layer class\n",
        "        self.conv = Conv2d(self.select_activation, self.optimizer, self.filter_num, self.input_channel, self.filter_size)\n",
        "        \n",
        "        ##Instantiation of activation function class\n",
        "        if self.select_activation == 'sigmoid':\n",
        "            self.activation_conv = Sigmoid()\n",
        "        elif self.select_activation == 'tanh':\n",
        "            self.activation_conv = Tanh()\n",
        "        elif self.select_activation == 'relu':\n",
        "            self.activation_conv = ReLU()\n",
        "            \n",
        "        ##Instantiation of pooling layer class\n",
        "        self.pool = MaxPool2D()   \n",
        "\n",
        "        ## Instantiation of smoothing class\n",
        "        self.flatten = Flatten()\n",
        "\n",
        "        ##Size before full connection\n",
        "        out_h, out_w = self._calc_out_shape(self.input_h, self.filter_size, self.input_w, self.filter_size, 'conv')\n",
        "        out_h, out_w = self._calc_out_shape(out_h, self.filter_size, out_w, self.filter_size, 'pool')\n",
        "        self.out_size = out_h * out_w * self.filter_num\n",
        "        \n",
        "        ##Initialization of N layer\n",
        "        self._initialize_n_layers()\n",
        "        \n",
        "        ##Get a mini-batch\n",
        "        get_mini_batch = GetMiniBatch(X, y, self.batch_size)\n",
        "        \n",
        "        for epoch in range(self.epochs):\n",
        "            print(\"epoch \", epoch + 1 , \" processing . . .\")\n",
        "            for mini_X_train,  mini_y_train in get_mini_batch:\n",
        "                self.X_ = mini_X_train\n",
        "                self.y_ = mini_y_train\n",
        "                \n",
        "                ##Forward propagation\n",
        "                ##Convolution layer\n",
        "                self.A = self.conv.forward(self.X_)\n",
        "                ##Activation function\n",
        "                self.Z = self.activation_conv.forward(self.A)\n",
        "                ##Pooling layer\n",
        "                self.P = self.pool.forward(self.Z)\n",
        "                ##Smoothing\n",
        "                self.F = self.flatten.forward(self.P)\n",
        "                \n",
        "                self.A = self.FC[0].forward(self.F)\n",
        "                self.Z = self.activation[0].forward(self.A)       \n",
        "                for n_layer in range(1, len(self.n_nodes) + 1):\n",
        "                    self.A = self.FC[n_layer].forward(self.Z)\n",
        "                    self.Z = self.activation[n_layer].forward(self.A)\n",
        "                \n",
        "                ##Backpropagation\n",
        "                self.dA, self.loss[epoch] = self.activation[len(self.n_nodes)].backward(self.Z, self.y_) ##Final layer, cross entropy error                \n",
        "                self.dZ = self.FC[len(self.n_nodes)].backward(self.dA) \n",
        "                for n_layer in reversed(range(0, len(self.n_nodes))): \n",
        "                    self.dA = self.activation[n_layer].backward(self.dZ)\n",
        "                    self.dZ = self.FC[n_layer].backward(self.dA)\n",
        "\n",
        "                ##Return of shape\n",
        "                self.dF = self.flatten.backward(self.dZ)\n",
        "                \n",
        "                ##Pooling layer\n",
        "                self.dP = self.pool.backward(self.dF)\n",
        "                ##Activation function\n",
        "                self.dA = self.activation_conv.backward(self.dP)\n",
        "                ##Convolution layer\n",
        "                self.dZ = self.conv.backward(self.dA)\n",
        "                \n",
        "    def predict(self,X):\n",
        "        ## Forward propagation\n",
        "        ##Convolution\n",
        "        self.A = self.conv.forward(X)\n",
        "        ##Activation function\n",
        "        self.Z = self.activation_conv.forward(self.A)\n",
        "        ##Pooling layer\n",
        "        self.P = self.pool.forward(self.Z)\n",
        "        ##Smoothing\n",
        "        self.F = self.flatten.forward(self.P)\n",
        "\n",
        "        self.A = self.FC[0].forward(self.F)\n",
        "        self.Z = self.activation[0].forward(self.A)    \n",
        "        for n_layer in range(1, len(self.n_nodes) + 1):\n",
        "            self.A = self.FC[n_layer].forward(self.Z)\n",
        "            self.Z = self.activation[n_layer].forward(self.A)\n",
        "        \n",
        "        return np.argmax(self.Z, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "55cba690",
      "metadata": {
        "id": "55cba690"
      },
      "outputs": [],
      "source": [
        "class FC:\n",
        "    \"\"\"\n",
        "    Number of nodes Fully connected layer from n_nodes1 to n_nodes2\n",
        "     Parameters\n",
        "     ----------\n",
        "     n_nodes1: int\n",
        "       Number of nodes in the previous layer\n",
        "     n_nodes2: int\n",
        "       Number of nodes in the later layer\n",
        "     initializer: instance of initialization method\n",
        "     optimizer: instance of optimization method\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        self.optimizer = optimizer\n",
        "        \n",
        "        ## Initialization\n",
        "         ## Initialize self.W and self.B using the initialr method\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.B = initializer.B(n_nodes2)\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        forward\n",
        "         Parameters\n",
        "         ----------\n",
        "         X: ndarray, shape (batch_size, n_nodes1) of the following form\n",
        "             input\n",
        "         Returns\n",
        "         ----------\n",
        "         A: ndarray, shape (batch_size, n_nodes2) of the following form\n",
        "             output        \"\"\"        \n",
        "        self.X = X\n",
        "        \n",
        "        return np.dot(self.X, self.W) + self.B\n",
        "    \n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "       Backward\n",
        "         Parameters\n",
        "         ----------\n",
        "         dA: ndarray, shape (batch_size, n_nodes2) of the following form\n",
        "             Gradient flowing from behind\n",
        "         Returns\n",
        "         ----------\n",
        "         dZ: ndarray, shape (batch_size, n_nodes1) of the following form\n",
        "             Gradient to flow forward\n",
        "        \"\"\"\n",
        "        self.dB = dA\n",
        "        self.dW = np.dot(self.X.T, dA)\n",
        "        dZ = np.dot(dA, self.W.T)\n",
        "\n",
        "       ## Update\n",
        "        self = self.optimizer.update(self)\n",
        "        \n",
        "        return dZ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "0f77b831",
      "metadata": {
        "id": "0f77b831"
      },
      "outputs": [],
      "source": [
        "class XavierInitializer:\n",
        "    \"\"\"\n",
        "    Xavier class\n",
        "    \"\"\"\n",
        "    def __init__(self, filter_num=None, input_channel=None, filter_size=None):        \n",
        "        self.filter_num = filter_num\n",
        "        self.input_channel = input_channel\n",
        "        self.filter_size = filter_size \n",
        "        \n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization\n",
        "         Parameters\n",
        "         ----------\n",
        "         n_nodes1: int\n",
        "           Number of nodes in the previous layer\n",
        "         n_nodes2: int\n",
        "           Number of nodes in the later layer\n",
        "\n",
        "         Returns\n",
        "         ----------\n",
        "         W: ndarray, shape (n_nodes1, n_nodes2) of the following form\n",
        "             weight\n",
        "        \"\"\"     \n",
        "        if self.filter_num and self.input_channel and self.filter_size is not None: ## Convolution layer\n",
        "            W = np.random.randn(self.filter_num, self.input_channel, self.filter_size, self.filter_size)\n",
        "        else: ## Fully connected layer\n",
        "            W = np.random.randn(n_nodes1, n_nodes2) / np.sqrt(n_nodes1)       \n",
        "        \n",
        "        return W\n",
        "        \n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        Bias initialization\n",
        "         Parameters\n",
        "         ----------\n",
        "         n_nodes2: int\n",
        "           Number of nodes in the later layer\n",
        "\n",
        "         Returns\n",
        "         ----------\n",
        "         B: ndarray, shape (n_nodes2,) of the following form\n",
        "             bias\n",
        "        \"\"\"       \n",
        "        if self.filter_num and self.input_channel and self.filter_size is not None: ## Convolution layer\n",
        "            B = np.random.randn(self.filter_num)\n",
        "        else: ## Fully connected layer\n",
        "            B = np.zeros(n_nodes2)\n",
        "        \n",
        "        return B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "5c81ee51",
      "metadata": {
        "id": "5c81ee51"
      },
      "outputs": [],
      "source": [
        "class HeInitializer:\n",
        "    \"\"\"\n",
        "   He class\n",
        "    \"\"\"\n",
        "    def __init__(self, filter_num=None, input_channel=None, filter_size=None):        \n",
        "        self.filter_num = filter_num\n",
        "        self.input_channel = input_channel\n",
        "        self.filter_size = filter_size       \n",
        "        \n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Weight initialization\n",
        "         Parameters\n",
        "         ----------\n",
        "         n_nodes1: int\n",
        "           Number of nodes in the previous layer\n",
        "         n_nodes2: int\n",
        "           Number of nodes in the later layer\n",
        "\n",
        "         Returns\n",
        "         ----------\n",
        "         W: ndarray, shape (n_nodes1, n_nodes2) of the following form\n",
        "             weight\n",
        "        \"\"\"      \n",
        "        if self.filter_num and self.input_channel and self.filter_size is not None: ## Convolution layer\n",
        "            W = np.random.randn(self.filter_num, self.input_channel, self.filter_size, self.filter_size) * np.sqrt(2 / self.filter_num) \n",
        "        else: ## Fully connected layer\n",
        "            W = np.random.randn(n_nodes1, n_nodes2) * np.sqrt(2 / n_nodes1)\n",
        "    \n",
        "        return W\n",
        "        \n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        Bias initialization\n",
        "         Parameters\n",
        "         ----------\n",
        "         n_nodes2: int\n",
        "           Number of nodes in the later layer\n",
        "\n",
        "         Returns\n",
        "         ----------\n",
        "         B: ndarray, shape (n_nodes2,) of the following form\n",
        "             bias\n",
        "        \"\"\"      \n",
        "        if self.filter_num and self.input_channel and self.filter_size is not None: ## Convolution layer\n",
        "            B = np.random.randn(self.filter_num)\n",
        "        else: ## Fully connected layer\n",
        "            B = np.random.randn(n_nodes2)\n",
        "        \n",
        "        return B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "139fad31",
      "metadata": {
        "id": "139fad31"
      },
      "outputs": [],
      "source": [
        "class SGD:\n",
        "    \"\"\"\n",
        "    Stochastic gradient descent\n",
        "     Parameters\n",
        "     ----------\n",
        "     lr: Learning rate\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "        \n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases for a layer\n",
        "         Parameters\n",
        "         ----------\n",
        "         layer: Instance of the layer before update\n",
        "        \n",
        "         Returns\n",
        "         ----------\n",
        "         layer: Updated layer instance\n",
        "        \"\"\"\n",
        "        layer.W = layer.W - self.lr * layer.dW\n",
        "        layer.B = layer.B - self.lr * layer.dB.mean(axis=0)\n",
        "        \n",
        "        return layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ed12f9ff",
      "metadata": {
        "id": "ed12f9ff"
      },
      "outputs": [],
      "source": [
        "class AdaGrad:\n",
        "    \"\"\"\n",
        "    AdaGrad class\n",
        "     Parameters\n",
        "     ----------\n",
        "     alpha: Learning rate\n",
        "    \n",
        "     Attributes\n",
        "     -------------\n",
        "     lr: Learning rate\n",
        "     HW: int (initial value), ndarray\n",
        "     HB: int (initial value), ndarray\n",
        "    \"\"\"    \n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "        self.HW= 0 ## Initial value: 0\n",
        "        self.HB = 0 ## Initial value: 0      \n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases for a layer\n",
        "         Parameters\n",
        "         ----------\n",
        "         layer: instance\n",
        "             Instance of the layer before update\n",
        "\n",
        "         Returns\n",
        "         ----------\n",
        "         layer: instance\n",
        "             Updated tier instance\n",
        "        \"\"\"\n",
        "        ## Initialization\n",
        "        self.HW = np.zeros_like(layer.W)\n",
        "        self.HB = np.zeros_like(layer.B)\n",
        "        \n",
        "        ## update\n",
        "        self.HW = self.HW + (layer.dW**2) #/ layer.dB.shape[0]\n",
        "        self.HB = self.HB + (layer.dB**2).mean(axis=0)\n",
        "        layer.W = layer.W - self.lr * 1 / np.sqrt(self.HW + 1e-7) * layer.dW #/ layer.dB.shape[0]\n",
        "        layer.B = layer.B - self.lr * 1 / np.sqrt(self.HB + 1e-7) * layer.dB.mean(axis=0)\n",
        "        \n",
        "        return layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "9e9fbbc8",
      "metadata": {
        "id": "9e9fbbc8"
      },
      "outputs": [],
      "source": [
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        \n",
        "        self.Z = 1.0 / (1.0 + np.exp(-self.A))\n",
        "        \n",
        "        return self.Z\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "\n",
        "        return dZ * (1 - self.Z) * self.Z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "6069a5e8",
      "metadata": {
        "id": "6069a5e8"
      },
      "outputs": [],
      "source": [
        "class Tanh:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        \n",
        "        return np.tanh(self.A)\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        \n",
        "        return dZ * (1.0 - (np.tanh(self.A) ** 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "38f18cd7",
      "metadata": {
        "id": "38f18cd7"
      },
      "outputs": [],
      "source": [
        "class ReLU:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def forward(self, A):    \n",
        "        self.A = A\n",
        "      \n",
        "        return np.maximum(self.A, 0)\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "            \n",
        "        return np.where(self.A > 0, dZ, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "5e21f363",
      "metadata": {
        "id": "5e21f363"
      },
      "outputs": [],
      "source": [
        "class Softmax:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def forward(self, A):\n",
        "        \n",
        "        return np.exp(A) / np.sum(np.exp(A), axis=1, keepdims=True)\n",
        "    \n",
        "    def backward(self, Z, y):\n",
        "        \n",
        "        dA = Z - y\n",
        "        \n",
        "        ## Cross entropy error\n",
        "        batch_size = y.shape[0]\n",
        "        loss = -np.sum(y * np.log(Z)) / batch_size\n",
        "\n",
        "        return dA, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "dfdd1572",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfdd1572",
        "outputId": "3eeb1d09-93bf-406e-f039-0a1fe5f0c972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  1  processing . . .\n",
            "epoch  2  processing . . .\n",
            "epoch  3  processing . . .\n",
            "epoch  4  processing . . .\n",
            "epoch  5  processing . . .\n",
            "epoch  6  processing . . .\n",
            "epoch  7  processing . . .\n",
            "epoch  8  processing . . .\n",
            "epoch  9  processing . . .\n",
            "epoch  10  processing . . .\n"
          ]
        }
      ],
      "source": [
        "model = Scratch2dCNNClassifier(activation='relu', n_nodes=[400, 200, 100], n_output=10, lr=0.001, optimizer='sgd', filter_num=3, filter_size=3)\n",
        "model.fit(X_train, y_train,epochs=10, batch_size=20)\n",
        "y_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccffeef9",
      "metadata": {
        "id": "ccffeef9"
      },
      "outputs": [],
      "source": [
        "print(\"accuracy : {}\".format(accuracy_score(y_test, y_pred)))\n",
        "print(\"\\n\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "27fbed52",
      "metadata": {
        "id": "27fbed52"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import keras\n",
        "from keras import Sequential\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
        "from keras.datasets import mnist\n",
        "from keras.callbacks import TensorBoard\n",
        "def lenet(input_shape, num_classes):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(6, kernel_size=5, strides=(1, 1), input_shape=input_shape, activation=\"relu\"))\n",
        "  model.add(Conv2D(16, kernel_size=5, strides=(1, 1), activation=\"relu\"))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(120, activation=\"relu\") )\n",
        "  model.add(Dense(84, activation=\"relu\"))\n",
        "  model.add(Dense(num_classes, activation=\"softmax\"))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "66dade44",
      "metadata": {
        "id": "66dade44"
      },
      "outputs": [],
      "source": [
        "class MNISTDataset():\n",
        "  def __init__(self):\n",
        "    self.image_shape = (28, 28, 1)\n",
        "    self.num_classes = 10\n",
        "\n",
        "  def get_batch(self):\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "    X_train, X_test = [self.preprocess(d) for d in [X_train, X_test]]\n",
        "    y_train, y_test = [self.preprocess(d, label_data=True) for d in [y_train, y_test]]\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "  def preprocess(self, data, label_data=False):\n",
        "    if label_data:\n",
        "      data = keras.utils.to_categorical(data, self.num_classes)\n",
        "    else:\n",
        "      data = data.astype(\"float32\") / 255\n",
        "      shape = (data.shape[0],) + self.image_shape\n",
        "      data = data.reshape(shape)\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "b837102a",
      "metadata": {
        "id": "b837102a"
      },
      "outputs": [],
      "source": [
        "class Trainer():\n",
        "  def __init__(self, model, loss, optimizer, logdir=\"logdir\"):\n",
        "    self.target = model\n",
        "    self.target.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "    self.verbose = 1\n",
        "    self.log_dir = os.path.join(os.path.dirname(__file__), logdir)\n",
        "    if not os.path.exists(self.log_dir):\n",
        "      os.mkdir(self.log_dir)\n",
        "\n",
        "  def train(self, X_train, y_train, batch_size, epochs, validation_split):\n",
        "    self.target.fit(X_train, y_train, \n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_split=validation_split,\n",
        "                    callbacks=[TensorBoard(log_dir=self.log_dir)],\n",
        "                    verbose=self.verbose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "54d7515c",
      "metadata": {
        "id": "54d7515c"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    __file__\n",
        "except NameError:\n",
        "    __file__ = os.path.join(os.getcwd(),\"dummy\")\n",
        "else:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "7c7671f3",
      "metadata": {
        "id": "7c7671f3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "fc36c9f2",
      "metadata": {
        "id": "fc36c9f2"
      },
      "outputs": [],
      "source": [
        "def calc_out_shape_and_params(H, FH, W, FW, pad, stride, IC, FN):\n",
        "    out_h = int(1 + (H + 2*pad - FH) / stride)\n",
        "    out_w = int(1 + (W + 2*pad - FW) / stride)\n",
        "    \n",
        "    params = FH * FW * IC * FN + FN ## Finally add the bias term + FN\n",
        "        \n",
        "    activation_size = FN * out_h * out_w\n",
        "        \n",
        "    return out_h, out_w, params, activation_size\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "4cab5f2b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cab5f2b",
        "outputId": "522dc2dd-2f3b-4538-f11d-0116e888aff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output size(FN × OH × OW) : 6 × 142 × 142\n",
            "Activation Size : 120984\n",
            "Number of parameters : 168\n"
          ]
        }
      ],
      "source": [
        "##Input size\n",
        "H, W, IC = (144, 144, 3) # IC = input_channel\n",
        "\n",
        "##Filter size\n",
        "FH, FW, FC = (3, 3, 3) ## FC (unused)= filter_channel\n",
        "\n",
        "##Number of filters (The filter size of 6 channels specified in the problem is assumed to be the number of filters)\n",
        "FN = 6\n",
        "\n",
        "##Stride, padding\n",
        "stride, pad = 1, 0\n",
        "\n",
        "out_h, out_w, params, activation_size = calc_out_shape_and_params(H, FH, W, FW, pad, stride, IC, FN)\n",
        "\n",
        "print('Output size(FN × OH × OW) : {} × {} × {}'.format(FN, out_h, out_w))\n",
        "print('Activation Size : {}'.format(activation_size))\n",
        "print('Number of parameters : {}'.format(params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "71d03bc3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71d03bc3",
        "outputId": "c3607f0b-61fd-4a92-cf67-c6ff40abc548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output size(FN × OH × OW) : 48 × 58 × 58\n",
            "Activation Size : 161472\n",
            "Number of parameters : 10416\n"
          ]
        }
      ],
      "source": [
        "##Input size\n",
        "H, W, IC = (60, 60, 24) ## IC = input_channel\n",
        "\n",
        "##Filter size\n",
        "FH, FW, FC = (3, 3, 24) ## FC(unused) = filter_channel\n",
        "\n",
        "##Number of filters (The problem-specified filter size of 48 channels is assumed to be the number of filters)\n",
        "FN = 48\n",
        "\n",
        "##Stride, padding\n",
        "stride, pad = 1, 0\n",
        "\n",
        "out_h, out_w, params, activation_size = calc_out_shape_and_params(H, FH, W, FW, pad, stride, IC, FN)\n",
        "\n",
        "print('Output size(FN × OH × OW) : {} × {} × {}'.format(FN, out_h, out_w))\n",
        "print('Activation Size : {}'.format(activation_size))\n",
        "print('Number of parameters : {}'.format(params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "0222dcd3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0222dcd3",
        "outputId": "f6999306-5de0-48da-f3a9-105281bce5d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output size(FN × OH × OW) : 20 × 9 × 9\n",
            "Activation Size : 1620\n",
            "Number of parameters : 1820\n"
          ]
        }
      ],
      "source": [
        "##Input size\n",
        "H, W, IC = (20, 20, 10) ## IC = input_channel\n",
        "\n",
        "##Filter size\n",
        "FH, FW, FC = (3, 3, 10) ## FC(unused) = filter_channel\n",
        "\n",
        "##Number of filters (The 20-channel filter size specified in the problem is assumed to be the number of filters)\n",
        "FN = 20\n",
        "\n",
        "##Stride, padding\n",
        "stride, pad = 2, 0\n",
        "\n",
        "out_h, out_w, params, activation_size = calc_out_shape_and_params(H, FH, W, FW, pad, stride, IC, FN)\n",
        "\n",
        "print('Output size(FN × OH × OW) : {} × {} × {}'.format(FN, out_h, out_w))\n",
        "print('Activation Size : {}'.format(activation_size))\n",
        "print('Number of parameters : {}'.format(params))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}