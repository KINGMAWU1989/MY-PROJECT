{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JMbQUlqDkxWF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import shutil\n",
        "from shutil import copyfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "from tensorflow.keras import Model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from matplotlib.offsetbox import (TextArea, DrawingArea, OffsetImage,\n",
        "                                  AnnotationBbox)\n",
        "import matplotlib.patches as mpatches\n",
        "from sklearn.utils import shuffle\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjkOBq-vhG-K",
        "outputId": "5dfb9d86-73e9-4ddd-e68c-ad564f695b43"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "get sample dataframe with class labels"
      ],
      "metadata": {
        "id": "_v5R6vOznxDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"data.csv\")[['Image', 'Class']]\n",
        "display(df.head())\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "5Et8YMEQlR4J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "08299185-393d-4342-b14a-141085b09478"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    Image  Class\n",
              "0  Image1      0\n",
              "1  Image2      0\n",
              "2  Image3      1\n",
              "3  Image4      1\n",
              "4  Image5      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a0b40120-87a1-4bf0-bd60-38a27489edff\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Image1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Image2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Image3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Image4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Image5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a0b40120-87a1-4bf0-bd60-38a27489edff')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a0b40120-87a1-4bf0-bd60-38a27489edff button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a0b40120-87a1-4bf0-bd60-38a27489edff');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3762, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bckN_j_YdLWV",
        "outputId": "1531020c-5ecb-45d3-df31-9aa710ac166a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3762 entries, 0 to 3761\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Image   3762 non-null   object\n",
            " 1   Class   3762 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 58.9+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.0 | Rearrange Images into Training, Validation and Testing\n",
        "Split files into training, validation and testing. We are using training and validation files when training the model. And we will using testing files to evaluate the final model.\n",
        "Training: 0.8 proportion\n",
        "Validation: 0.1 proportion\n",
        "Testing: 0.1 proportion"
      ],
      "metadata": {
        "id": "JRHk0QUqoGku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Training + Validation with Testing Set\n",
        "def split_size(df, size):\n",
        "    return int(size * len(df))\n",
        "\n",
        "\n",
        "train_labels = df['Class'].values[:split_size(df, 0.8)]\n",
        "train_file_names = df['Image'].values[:split_size(df, 0.8)]\n",
        "\n",
        "val_labels = df['Class'].values[split_size(df, 0.8):split_size(df, 0.9)]\n",
        "val_file_names = df['Image'].values[split_size(df, 0.8):split_size(df, 0.9)]\n",
        "\n",
        "test_labels = df['Class'].values[split_size(df, 0.9):]\n",
        "test_file_names = df['Image'].values[split_size(df, 0.9):]"
      ],
      "metadata": {
        "id": "nzqv0vrxlR-d"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_array_labels(arr_image, arr_label):\n",
        "    arr_image_0 = arr_image[np.where(arr_label==0)]\n",
        "    arr_image_1 = arr_image[np.where(arr_label==1)]\n",
        "    return {'0':arr_image_0, '1':arr_image_1}"
      ],
      "metadata": {
        "id": "bQckYAqGlSSC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_arr_dict = split_array_labels(train_file_names, train_labels)\n",
        "val_arr_dict = split_array_labels(val_file_names, val_labels)\n",
        "test_arr_dict = split_array_labels(test_file_names, test_labels)"
      ],
      "metadata": {
        "id": "CD5-6r5olShh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create empyty directories of training, validation and testing"
      ],
      "metadata": {
        "id": "HrXqzepDpy3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_empty_directories(base_dir):\n",
        "    # in case you want to run it several times, delete the directory and create new one\n",
        "    check_exist_path = os.path.join(base_dir, '_MODELLING')\n",
        "    if os.path.isdir(check_exist_path):\n",
        "        shutil.rmtree(check_exist_path)\n",
        "        print(\"Remove old directories\")\n",
        "\n",
        "    for label in ['0','1']:\n",
        "        training_dir = os.path.join(base_dir, '_MODELLING', 'training', label)\n",
        "        validation_dir = os.path.join(base_dir, '_MODELLING', 'validation', label)\n",
        "        testing_dir = os.path.join(base_dir, '_MODELLING', 'testing', label)\n",
        "\n",
        "\n",
        "\n",
        "        os.makedirs(training_dir)\n",
        "        os.makedirs(validation_dir)\n",
        "        os.makedirs(testing_dir)\n",
        "    print(f\"Created empty  training, validation and testing directories\")\n",
        "create_empty_directories('/content/drive/myDrive/')"
      ],
      "metadata": {
        "id": "WKiWIzPflSmZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "a0b2146d-191e-428e-a825-567eae927faf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-414d2e82cfa8>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Created empty  training, validation and testing directories\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mcreate_empty_directories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/myDrive/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-414d2e82cfa8>\u001b[0m in \u001b[0;36mcreate_empty_directories\u001b[0;34m(base_dir)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 95] Operation not supported: '/content/drive/myDrive'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Created empty  training, validation and testing directories\n",
        "Split images by copying and pasting into their respective directories"
      ],
      "metadata": {
        "id": "R3MUAIqJqCNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(SOURCE_DIR, train_arr_dict, val_arr_dict, test_arr_dict):\n",
        "    for label in tqdm(['0','1']):\n",
        "        for file_name in train_arr_dict[label]:\n",
        "            file_name = f\"{file_name}.jpg\"\n",
        "            source = os.path.join(SOURCE_DIR, 'data', 'data', file_name)\n",
        "            destination = os.path.join('/content/drive', '_MODELLING', 'training', label, file_name)\n",
        "            copyfile(source, destination)\n",
        "\n",
        "        for file_name in val_arr_dict[label]:\n",
        "            file_name = f\"{file_name}.jpg\"\n",
        "            source = os.path.join(SOURCE_DIR, 'data', 'data', file_name)\n",
        "            destination = os.path.join('/content/drive', '_MODELLING', 'validation', label, file_name)\n",
        "            copyfile(source, destination)\n",
        "\n",
        "        for file_name in test_arr_dict[label]:\n",
        "            file_name = f\"{file_name}.jpg\"\n",
        "            source = os.path.join(SOURCE_DIR, 'data', 'data', file_name)\n",
        "            destination = os.path.join('/content/drive', '_MODELLING', 'testing', label, file_name)\n",
        "            copyfile(source, destination)\n",
        "    print(f\"Created training, validation and testing directories containing images\")\n",
        "\n",
        "split_data(base_dir,train_arr_dict,val_arr_dict,test_arr_dict)"
      ],
      "metadata": {
        "id": "ntjEhNnGlSx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.0 | Show Number of Images Per Category Label"
      ],
      "metadata": {
        "id": "dev81-P0ruwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir  = '/content/drive/_MODELLING'\n",
        "os.chdir(base_dir)\n",
        "\n",
        "mytrain_test_list = []\n",
        "mylabels_list = []\n",
        "myitem_list = []\n",
        "vis_images = []; vis_labels =[]\n",
        "\n",
        "train_test_list = tf.io.gfile.listdir(base_dir)\n",
        "for train_test in train_test_list:\n",
        "    path1 = os.path.join(base_dir, train_test)\n",
        "    label_list = tf.io.gfile.listdir(path1)\n",
        "    for label in label_list:\n",
        "        my_path = os.path.join(path1, label)\n",
        "        item_files = os.listdir(my_path)\n",
        "\n",
        "        mytrain_test_list.append(train_test)\n",
        "        mylabels_list.append(label)\n",
        "        myitem_list.append(len(item_files))\n",
        "\n",
        "        my_files = item_files[0:5]\n",
        "        for file in my_files:\n",
        "            vis_images.append(os.path.join(my_path, file))\n",
        "            vis_labels.append(label)\n",
        "\n",
        "pd.DataFrame({'Tran Test':mytrain_test_list, 'Labels':mylabels_list, \\\n",
        "              'Number of Items':myitem_list})"
      ],
      "metadata": {
        "id": "DeYUVGOqrdNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.0 | Show Sample Images of Each Label\n",
        "Visualising some images of each label"
      ],
      "metadata": {
        "id": "NwcHt_W1r-LP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get first 10 images for visualisation\n",
        "vis_images = vis_images[:10]\n",
        "vis_labels = vis_labels[:10]\n",
        "\n",
        "plt.figure(figsize=(12,7))\n",
        "for i in range(len(vis_labels)):\n",
        "    plt.subplot(2,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    img = mpimg.imread(vis_images[i])\n",
        "    plt.imshow(img)\n",
        "\n",
        "    if vis_labels[i] == '0':\n",
        "        my_label = 'No Tumor'\n",
        "    elif vis_labels[i] == '1':\n",
        "        my_label = 'With Tumor'\n",
        "\n",
        "    plt.xlabel(my_label)\n",
        "    plt.suptitle(f\"Classifying 2 Types of Image Labels\",fontsize=18, fontweight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f101E6XbrdUc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "cb925249-2f61-4ce3-a766-6e9760bc4fac"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-f271762d978a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get first 10 images for visualisation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvis_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvis_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvis_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvis_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vis_images' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.0 | Show Augmented Images, That Can Reduce Overfitting\n",
        "In order to train neural networks that will be used in real-world applications, data augmentation is a crucial step. We can improve our model's ability to generalise and produce more precise predictions on data that it was not trained on by using data augmentation.\n",
        "\n",
        "Data augmentation will operate concurrently with the other layers of your application on-device and will receive GPU acceleration."
      ],
      "metadata": {
        "id": "2XEWP56mtEXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_ImageDataGenerator(vis_images, vis_labels, image_index):\n",
        "    #Loads image in from the set image path\n",
        "    class_label = vis_labels[image_index]\n",
        "    img = tf.keras.preprocessing.image.load_img(vis_images[image_index], target_size= (250,250))\n",
        "    img_tensor = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "\n",
        "    #Creates our batch of one image\n",
        "    def show_image(datagen, param):\n",
        "        pic = datagen.flow(img_tensor, batch_size =1)\n",
        "        plt.figure(figsize=(10,3.5))\n",
        "        #Plots our figures\n",
        "        for i in range(1,4):\n",
        "            plt.subplot(1, 3, i)\n",
        "            batch = pic.next()\n",
        "            image_ = batch[0].astype('uint8')\n",
        "            plt.imshow(image_)\n",
        "        plt.suptitle(f\"Class: {class_label} \\n Image Generator ({param})\",fontsize=18, fontweight='bold')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    datagen = ImageDataGenerator(rotation_range=30)\n",
        "    show_image(datagen, \"rotation_range=30\")\n",
        "\n",
        "    datagen = ImageDataGenerator(width_shift_range=0.2)\n",
        "    show_image(datagen, \"width_shift_range=0.2\")\n",
        "\n",
        "    datagen = ImageDataGenerator(zoom_range=0.2)\n",
        "    show_image(datagen, \"zoom_range=0.2\")\n",
        "\n",
        "    datagen = ImageDataGenerator(horizontal_flip=True)\n",
        "    show_image(datagen, \"horizontal_flip=True\")\n",
        "\n",
        "show_ImageDataGenerator(vis_images, vis_labels, image_index = 5)"
      ],
      "metadata": {
        "id": "EFKT4UGcrdnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_ImageDataGenerator(vis_images, vis_labels, image_index = 2)"
      ],
      "metadata": {
        "id": "6J0bA_vgrdsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.0 | Apply Image Augmentation using Image Data Generator\n",
        "tf.keras.applications.MobileNetV2 for use as your base model. This model expects pixel values in [-1, 1], but at this point, the pixel values in your images are in [0, 255]. To rescale them, we can rescale by 1./127.5."
      ],
      "metadata": {
        "id": "6hwKkWXitv0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_generators(TRAINING_DIR, VALIDATION_DIR, TEST_DIR):\n",
        "\n",
        "    # Instantiate the ImageDataGenerator class (don't forget to set the arguments to augment the images)\n",
        "    train_datagen = ImageDataGenerator(rescale=1./127.5,\n",
        "                                     rotation_range=30,\n",
        "                                     width_shift_range=0.2,\n",
        "                                     height_shift_range=0.2,\n",
        "                                     shear_range=0.2,\n",
        "                                     zoom_range=0.2,\n",
        "                                     horizontal_flip=True,\n",
        "                                     fill_mode='nearest')\n",
        "\n",
        "    # Pass in the appropriate arguments to the flow_from_directory method\n",
        "    train_generator = train_datagen.flow_from_directory(directory=TRAINING_DIR,\n",
        "                                                      batch_size=32,\n",
        "                                                      class_mode='binary',\n",
        "                                                      target_size=(150, 150))\n",
        "\n",
        "    # Instantiate the ImageDataGenerator class (don't forget to set the rescale argument)\n",
        "    valid_or_test_datagen = ImageDataGenerator(rescale=1./127.5)\n",
        "\n",
        "    # Pass in the appropriate arguments to the flow_from_directory method\n",
        "    validation_generator = valid_or_test_datagen.flow_from_directory(directory=VALIDATION_DIR,\n",
        "                                                                batch_size=32,\n",
        "                                                                class_mode='binary',\n",
        "                                                                target_size=(150, 150))\n",
        "\n",
        "    test_generator = valid_or_test_datagen.flow_from_directory(directory=TEST_DIR,\n",
        "                                                                batch_size=32,\n",
        "                                                                class_mode='binary',\n",
        "                                                                target_size=(150, 150))\n",
        "    return train_generator, validation_generator, test_generator"
      ],
      "metadata": {
        "id": "HMW2puwwrd1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_dir = os.path.join(base_dir, 'training')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "testing_dir = os.path.join(base_dir, 'testing')\n",
        "\n",
        "print(testing_dir)"
      ],
      "metadata": {
        "id": "ve7RHydard_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator, validation_generator, test_generator = train_val_generators(training_dir, validation_dir, testing_dir)"
      ],
      "metadata": {
        "id": "Z7Ssn1hIreEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.0 | Get Transfer Learning Model - MobileNetV2\n",
        "MobileNet-v2 is a convolutional neural network consisting of 53 layers deep. The ImageNet database contains a pretrained version of the network that has been trained on more than a million images [1]. The pretrained network can categorise photos into 1000 different object categories. It is good to have a good sense of initialisation of parameters. So we will freeze the top layers of MovileNetV2 first and train on the last output layers."
      ],
      "metadata": {
        "id": "_ylzsLmoud8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = tf.keras.applications.MobileNetV2(input_shape=(150, 150, 3),\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "base_model.trainable = False\n",
        "last_output = base_model.output\n",
        "num_trainable_params = sum([w.shape.num_elements() for w in base_model.trainable_weights])\n",
        "\n",
        "print(f\"There are {num_trainable_params:,} trainable parameters in this model.\")\n",
        "print(f\"The pretrained model has type: {type(base_model)}\")\n"
      ],
      "metadata": {
        "id": "w4dV2VixreKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.0 | Building Deep Neural Network Architecture with MobileNetV2"
      ],
      "metadata": {
        "id": "qBmQ-w64vBDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transfer_learning(last_output, pre_trained_model):\n",
        "    # Flatten the output layer to 1 dimension\n",
        "    x = tf.keras.layers.Flatten()(last_output)\n",
        "    # Add a fully connected layer with 1024 hidden units and ReLU activation\n",
        "    x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
        "    # Add a dropout rate of 0.6\n",
        "    x = tf.keras.layers.Dropout(0.6)(x)\n",
        "    # Add a final sigmoid layer for classification\n",
        "    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    # Create the complete model by using the Model class\n",
        "    model = Model(inputs=pre_trained_model.input, outputs=x)\n",
        "\n",
        "    return model\n",
        "\n",
        "model = transfer_learning(last_output, base_model)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "UsTl064xreQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = transfer_learning(last_output, base_model)\n",
        "\n",
        "print(f\"Total Trainable Variables: {len(model.trainable_variables)}\")"
      ],
      "metadata": {
        "id": "N-L4Oy_9reV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This callback will stop the training when there is no improvement in the validation loss for three consecutive epochs."
      ],
      "metadata": {
        "id": "XHslJzvCvtQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)"
      ],
      "metadata": {
        "id": "exg1tHYmrebV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0003),\n",
        "            loss = 'binary_crossentropy',\n",
        "            metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "pMidS1C3reg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_generator,\n",
        "                    epochs=10,\n",
        "                    validation_data=validation_generator,\n",
        "                    callbacks=[callback])"
      ],
      "metadata": {
        "id": "CTPWt-XWwHyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vis_evaluation(history_dict, model_name):\n",
        "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 10))\n",
        "    epochs = range(1, len(history_dict['accuracy'])+1)\n",
        "\n",
        "    def get_gradient(y_arr, epochs):\n",
        "        return round((y_arr[-1] - y_arr[0]) / (epochs[-1] - epochs[0]),2)\n",
        "\n",
        "    def vis_sub_evaluation(n, Accuracy, train_acc, val_acc, epochs):\n",
        "        axs[n].plot(epochs, train_acc, label=f'Training {Accuracy}', ls='--')\n",
        "        axs[n].plot(epochs, val_acc, label=f'Validation {Accuracy}', ls='dotted')\n",
        "\n",
        "        axs[n].set_title(f'Training and Validation {Accuracy}')\n",
        "        axs[n].set_xlabel('Epochs')\n",
        "        axs[n].set_ylabel(Accuracy)\n",
        "\n",
        "        handles, labels = axs[n].get_legend_handles_labels()\n",
        "        m_patch = mpatches.Patch(color='grey',label='m: gradient')\n",
        "        handles.append(m_patch)\n",
        "        axs[n].legend(handles=handles)\n",
        "\n",
        "        def annotate_box(train_acc):\n",
        "            return AnnotationBbox(TextArea(f\"m = {get_gradient(train_acc, epochs)}\"), (epochs[-1], train_acc[-1]),\n",
        "                            xybox=(20, 20),\n",
        "                            xycoords='data',\n",
        "                            boxcoords=\"offset points\",\n",
        "                            arrowprops=dict(arrowstyle=\"->\"))\n",
        "        axs[n].add_artist(annotate_box(train_acc))\n",
        "        axs[n].add_artist(annotate_box(val_acc))\n",
        "\n",
        "    train_acc = history_dict['accuracy']\n",
        "    val_acc = history_dict['val_accuracy']\n",
        "    vis_sub_evaluation(0, 'Accuracy', train_acc, val_acc, epochs)\n",
        "\n",
        "    train_loss = history_dict['loss']\n",
        "    val_loss = history_dict['val_loss']\n",
        "    vis_sub_evaluation(1, 'Loss', train_loss, val_loss, epochs)\n",
        "\n",
        "    plt.suptitle(f\"Performance Evaluation of {model_name}\",fontsize=18, fontweight='bold')\n",
        "    plt.show()\n",
        "\n",
        "history_dict_1 = history.history\n",
        "vis_evaluation(history_dict_1, 'Transfer Learning MobileNetV2')"
      ],
      "metadata": {
        "id": "NWeENLaXwH7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.0 | Fine Tuning Transfer Learning Model"
      ],
      "metadata": {
        "id": "K_vnvtYiwgje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.trainable = True\n",
        "# Let's take a look to see how many layers are in the base model\n",
        "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "\n",
        "# Fine-tune from this layer onwards\n",
        "fine_tune_at = 100\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable = False\n",
        "\n",
        "print(f\"Total Trainable Variables: {len(model.trainable_variables)}\")\n",
        "Number of layers in the base model:  154\n",
        "Total Trainable Variables: 58"
      ],
      "metadata": {
        "id": "cRrsZ13jwIA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001/10),\n",
        "            loss = 'binary_crossentropy',\n",
        "            metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "jthw0V0ewIF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_fine = model.fit(train_generator,\n",
        "                         epochs=15,\n",
        "                         initial_epoch=history.epoch[-1],\n",
        "                         validation_data=validation_generator)"
      ],
      "metadata": {
        "id": "NDe_210qwILE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_dict_2 = history_fine.history\n",
        "vis_evaluation(history_dict_2, 'Fine-Tuned Transfer Learning MobileNetV2')"
      ],
      "metadata": {
        "id": "Tjc2DBHSwIQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.0 | Evaluation on Unseen Data"
      ],
      "metadata": {
        "id": "s-y8rrUgxK7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(test_generator)\n",
        "print('Test accuracy :', accuracy)"
      ],
      "metadata": {
        "id": "3eHAEI8iwIVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.0 | Visualise Predictions on Unseen Data"
      ],
      "metadata": {
        "id": "p3-Ep4wMyV4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_paths = [] ; selected_labels = []\n",
        "testing_path_0 = os.path.join(base_dir, 'testing', '0')\n",
        "for x in random.sample(os.listdir(testing_path_0),10):\n",
        "    selected_paths.append(os.path.join(testing_path_0, x))\n",
        "    selected_labels.append(0)\n",
        "\n",
        "testing_path_1 = os.path.join(base_dir, 'testing', '1')\n",
        "for x in random.sample(os.listdir(testing_path_1),10):\n",
        "    selected_paths.append(os.path.join(testing_path_1, x))\n",
        "    selected_labels.append(1)"
      ],
      "metadata": {
        "id": "rOa3H_8jwIc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = shuffle(selected_paths, selected_labels, random_state=0)"
      ],
      "metadata": {
        "id": "HvUGuQMowIoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for vis_image, y_true in zip(X, y):\n",
        "    img = image.load_img(vis_image, target_size=(150, 150))\n",
        "    x = image.img_to_array(img)\n",
        "    x /= 127.5\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    preprocess_images = np.vstack([x])\n",
        "    classes = model.predict(preprocess_images, batch_size=10)\n",
        "    score = tf.nn.sigmoid(classes[0])\n",
        "    if classes[0]>0.5:\n",
        "        predicted_label = 1\n",
        "    else:\n",
        "        predicted_label =0\n",
        "    plt.title(f'True Label: {y_true} \\n Predicted Label: {predicted_label} with a {100 * np.max(score):.2f} percent confidence.')\n",
        "    plt.imshow(img)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "f1St4AHzypdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.0 | What Computer see during training convolutional images"
      ],
      "metadata": {
        "id": "eC3tjEEwy5Nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a new Model that will take an image as input, and will output\n",
        "# intermediate representations for all layers in the previous model\n",
        "successive_outputs = [layer.output for layer in model.layers]\n",
        "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
        "\n",
        "# Prepare a random input image from the training set.\n",
        "adult_img_files = vis_images[:5]\n",
        "child_img_files = vis_images[5:10]\n",
        "img_path = random.choice(adult_img_files + child_img_files)\n",
        "img = load_img(img_path, target_size=(150, 150))  # this is a PIL image\n",
        "x   = img_to_array(img)                           # Numpy array with shape (150, 150, 3)\n",
        "x   = x.reshape((1,) + x.shape)                   # Numpy array with shape (1, 150, 150, 3)\n",
        "\n",
        "# Scale by 1/255\n",
        "x /= 255.0\n",
        "\n",
        "# Run the image through the network, thus obtaining all\n",
        "# intermediate representations for this image.\n",
        "successive_feature_maps = visualization_model.predict(x)\n",
        "\n",
        "# These are the names of the layers, so you can have them as part of our plot\n",
        "layer_names = [layer.name for layer in model.layers]\n",
        "\n",
        "# Display the representations\n",
        "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
        "\n",
        "  if len(feature_map.shape) == 4:\n",
        "\n",
        "    #-------------------------------------------\n",
        "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
        "    #-------------------------------------------\n",
        "    n_features = feature_map.shape[-1]  # number of features in the feature map\n",
        "    size       = feature_map.shape[ 1]  # feature map shape (1, size, size, n_features)\n",
        "\n",
        "    # Tile the images in this matrix\n",
        "    display_grid = np.zeros((size, size * n_features))\n",
        "\n",
        "    #-------------------------------------------------\n",
        "    # Postprocess the feature to be visually palatable\n",
        "    #-------------------------------------------------\n",
        "    for i in range(n_features):\n",
        "        x  = feature_map[0, :, :, i]\n",
        "        x -= x.mean()\n",
        "        x /= x.std ()\n",
        "        x *=  64\n",
        "        x += 128\n",
        "        x  = np.clip(x, 0, 255).astype('uint8')\n",
        "        display_grid[:, i * size : (i + 1) * size] = x # Tile each filter into a horizontal grid\n",
        "\n",
        "    #-----------------\n",
        "    # Display the grid\n",
        "    #-----------------\n",
        "    scale = 20. / n_features\n",
        "    plt.figure( figsize=(scale * n_features, scale) )\n",
        "    plt.title ( layer_name )\n",
        "    plt.grid  ( False )\n",
        "    plt.imshow( display_grid, aspect='auto', cmap='viridis' )"
      ],
      "metadata": {
        "id": "5EKc3-u0ypk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.0 | Test Your Own Images Finally, you can enjoy the model prediction by using your own images."
      ],
      "metadata": {
        "id": "VHCPcb-1zblH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_your_prediction(YOUR_IMAGE_PATH = None):\n",
        "    if YOUR_IMAGE_PATH == None:\n",
        "        YOUR_IMAGE_PATH = '/kaggle/working/_MODELLING/testing/1/Image3702.jpg'\n",
        "\n",
        "    img = image.load_img(YOUR_IMAGE_PATH, target_size=(150, 150))\n",
        "    plt.imshow(img)\n",
        "    x = image.img_to_array(img)\n",
        "    x /= 127.5\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "\n",
        "    images = np.vstack([x])\n",
        "    classes = model.predict(images, batch_size=10)\n",
        "    score = tf.nn.sigmoid(classes[0])\n",
        "\n",
        "    class_name = train_generator.class_indices\n",
        "    class_name_inverted = {y: x for x, y in class_name.items()}\n",
        "\n",
        "    if classes[0]>0.5:\n",
        "        print(f\" This image most likely belongs to '{class_name_inverted[1]}' (With Tumor) at {100 * np.max(score):.2f} percent confidence.\")\n",
        "    else:\n",
        "        print(f\" This image most likely belongs to '{class_name_inverted[0]}' (Without Tumor) at {100 * np.max(score):.2f} percent confidence.\")\n",
        "\n",
        "make_your_prediction(YOUR_IMAGE_PATH = None)"
      ],
      "metadata": {
        "id": "z9MJEZ0-yprx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Author: Morris Lee 28/9/2022\n",
        "\n",
        "References\n",
        "Deng J, Dong W, Socher R, Li L-J, Li K, Fei-Fei L. Imagenet: A large-scale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition. 2009. p. 24855.The end.\n",
        "If you find this notebook useful useful. Please  Upvote and Comment down below"
      ],
      "metadata": {
        "id": "t9C2gfBcz-Ct"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j0F11UaLyp9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HMRu7Wg1yqFK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}