{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "540aebe2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlxtend'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KMeans\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlxtend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_decision_regions\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ListedColormap\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mlxtend'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.sparse as sp\n",
    "import statsmodels.api as sm\n",
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c043031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def smooth_curve(x):\n",
    "    \"\"\"\n",
    "     Used to smooth the graph of a loss function\n",
    "    Reference: http://glowingpython.blogspot.jp/2012/02/convolution-with-numpy.html\n",
    "    \"\"\"\n",
    "    window_len = 11\n",
    "    s = np.r_[x[window_len-1:0:-1], x, x[-1:-window_len:-1]]\n",
    "    w = np.kaiser(window_len, 2)\n",
    "    y = np.convolve(w/w.sum(), s, mode='valid')\n",
    "    return y[5:len(y)-5]\n",
    "\n",
    "\n",
    "def shuffle_dataset(x, t):\n",
    "    \"\"\"\n",
    "    Perform a shuffle of the dataset\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : training data\n",
    "    t : Teacher data\n",
    "    Returns\n",
    "    -------\n",
    "    x, t : shuffled training and teacher data\n",
    "    \"\"\"\n",
    "    permutation = np.random.permutation(x.shape[0])\n",
    "    x = x[permutation,:] if x.ndim == 2 else x[permutation,:,:,:]\n",
    "    t = t[permutation]\n",
    "\n",
    "    return x, t\n",
    "\n",
    "def conv_output_size(input_size, filter_size, stride=1, pad=0):\n",
    "    return (input_size + 2*pad - filter_size) / stride + 1\n",
    "\n",
    "\n",
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : input data consisting of a 4D array of (number of data, channel, height, width)\n",
    "    filter_h : height of the filter\n",
    "    filter_w : width of the filter\n",
    "    stride : stride\n",
    "    pad : padding\n",
    "    Returns\n",
    "    -------\n",
    "    col : 2D array\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col\n",
    "\n",
    "\n",
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    col :\n",
    "    input_shape : Shape of input data (example)：(10, 1, 28, 28)）\n",
    "    filter_h :\n",
    "    filter_w\n",
    "    stride\n",
    "    pad\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d02734",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_length = 100\n",
    "n_sample = 100000\n",
    "anomaly_noise = np.random.randn(data_length)    \n",
    "nomaly_noise  = np.random.randn(data_length)/2  \n",
    "t = np.linspace(0,10,data_length)\n",
    "x_anomaly = np.array([np.sin(t) + anomaly_noise for i in range(n_sample//2)])\n",
    "x_nomaly  = np.array([np.sin(t) + nomaly_noise  for i in range(n_sample//2)])\n",
    "X = np.concatenate([x_anomaly,x_nomaly])\n",
    "y = np.concatenate([np.ones(n_sample//2),np.zeros(n_sample//2)])\n",
    "rand_idx = np.arange(n_sample)\n",
    "np.random.shuffle(rand_idx)\n",
    "X = X[rand_idx,:]\n",
    "y = y[rand_idx]\n",
    "plt.plot(t,x_anomaly[0,:])\n",
    "plt.plot(t,x_nomaly[0,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13915002",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_1d:\n",
    "    def __init__(self, p=0, s=1):\n",
    "        self.padding = p\n",
    "        self.stride = s\n",
    "    \n",
    "    def forward(self, X, filter_size):\n",
    "        self.X = X\n",
    "        self.input_size = len(X)\n",
    "        self.filter_size = filter_size\n",
    "        self.W = np.random.randint(-2,2,3) \n",
    "        self.B = np.array([1])\n",
    "        # If the input is two-dimensional, calculate HW respectively\n",
    "        self.output_size = ((self.input_size + self.p*2 - self.filter_size) / self.stride) + 1\n",
    "        \n",
    "        \n",
    "    \n",
    "    def backward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7c0ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = np.random.randint(0, 10, 10)\n",
    "ww = np.random.randint(-2,2, 3)\n",
    "print(XX)\n",
    "print(ww)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eeed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_out_shape(Nin, F, P=0, S=1):\n",
    "    out = ((Nin + 2*P - F) / S ) + 1\n",
    "    return out\n",
    "calc_out_shape(100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b2a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples\n",
    "x = np.array([1, 2, 3, 4])\n",
    "y = np.array([45,70])\n",
    "w = np.array([3, 5, 7])\n",
    "a = np.empty((2, 3))\n",
    "indexes0 = np.array([0, 1, 2]).astype(np.int64)\n",
    "indexes1 = np.array([1, 2, 3]).astype(np.int64)\n",
    "a[0] = x[indexes0]*w # x[indexes0]は([1, 2, 3])である\n",
    "a[1] = x[indexes1]*w # x[indexes1]は([2, 3, 4])である\n",
    "a = a.sum(axis=1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec39c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Forward(X, W, B=1, P=0, S=1):\n",
    "    \n",
    "    lenX = X.shape[0]\n",
    "    lenW = W.shape[0]\n",
    "    \n",
    "    Nout = lenX - lenW +1\n",
    "\n",
    "    index_list = np.zeros((Nout, lenW)).astype(np.int64)\n",
    "\n",
    "    for i in range(Nout):\n",
    "        index_list[i] = (np.arange(i, lenW + i ))\n",
    "\n",
    "    output = np.dot(X[index_list], W) + B\n",
    "    \n",
    "    return output\n",
    "\n",
    "def cross_entropy_loss(pred, label):\n",
    "    return label - pred\n",
    "    \n",
    "def backward(lr, X, W, loss):\n",
    "    lenX = X.shape[0]\n",
    "    lenW = W.shape[0]\n",
    "    \n",
    "    Nout = lenX - lenW +1\n",
    "\n",
    "    index_list = np.zeros((Nout, lenW)).astype(np.int64)\n",
    "\n",
    "    for i in range(Nout):\n",
    "        index_list[i] = (np.arange(i, lenW + i ))\n",
    "        \n",
    "        \n",
    "    dB = np.sum(loss)\n",
    "    dW = np.dot(loss, X[index_list])\n",
    "\n",
    "    dX = np.zeros((lenX,))\n",
    "    for i,index in enumerate(index_list):\n",
    "        dX[index] += loss[i] * W\n",
    "        print(dX)\n",
    "                \n",
    "            \n",
    "    #print(dX)\n",
    "    return dW, dB, dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a0f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = Forward(x,w)\n",
    "loss = cross_entropy_loss(pred, y)\n",
    "dW, dB, dX = backward(0.01,x,w,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f00454",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"forward_result : {}\".format(pred))\n",
    "print(\"loss : {}\".format(loss))\n",
    "print(\"backward_result\")\n",
    "print(\"delta_b : {}\".format(dB))\n",
    "print(\"delta_w : {}\".format(dW))\n",
    "print(\"delta_x : {}\".format(dX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6661e031",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) \n",
    "ww = np.ones((3, 2, 3)) \n",
    "bb = np.array([1, 2, 3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5689bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1d:\n",
    "    def __init__(self,W, B, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.B = B\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self, X):\n",
    "        print(X.shape)\n",
    "        self.FN, self.C, self.FS = self.W.shape \n",
    "        self.C, self.S = X.shape\n",
    "        #print(self.F, self.FS)\n",
    "        self.Nout = int(((self.S + 2*self.pad - self.FS) / self.stride) + 1)\n",
    "        self.reW= self.W.reshape(self.FN,-1).T\n",
    "        print(self.reW)\n",
    "        print(\"reWのshape:\\n{}\".format(self.reW.shape))\n",
    "        self.output = np.zeros((self.S,self.Nout))\n",
    "        self.openX = np.zeros((self.Nout,self.FN*self.C))\n",
    "        self.index_list = np.zeros((self.Nout, self.FN)).astype(np.int64)\n",
    "        for i in range(self.Nout):\n",
    "            self.index_list[i] = np.arange(i, self.FN + i )\n",
    "        print(\"test\")\n",
    "        print(self.openX[:,self.index_list[0]])\n",
    "        print(X[0])\n",
    "        for k,v in enumerate(self.index_list):\n",
    "            for k2,v2 in enumerate(X):\n",
    "                #print(k,v,k2,v2)\n",
    "                if k2 == 0:\n",
    "                    self.openX[k][v - k] = v2[v]\n",
    "                elif k2 == 1:\n",
    "                    self.openX[k][v + len(v) - k] = v2[v]\n",
    "                else:\n",
    "                    self.openX[k][v + len(v)*k2 - k] = v2[v]\n",
    "                \n",
    "        print(self.openX)\n",
    "       \n",
    "        self.output = np.dot(self.openX, self.reW) + self.B\n",
    "\n",
    "        \n",
    "        return self.output.T\n",
    "    \n",
    "    \n",
    "    def cross_entropy_loss(self, pred, label):\n",
    "        return label - pred\n",
    "\n",
    "    def backward(self, X, W, loss):\n",
    "        dB = np.sum(loss)\n",
    "        dW = np.dot(loss,self.openX).reshape(self.W.shape)\n",
    "        dX = np.zeros(X.shape)\n",
    "        \n",
    "        calc = np.dot(self.W.T, loss).T\n",
    "        for k,v in enumerate(self.index_list):\n",
    "            dX[:,v] += calc[k]\n",
    "\n",
    "        return dW, dB, dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7ed198",
   "metadata": {},
   "outputs": [],
   "source": [
    "trap = CNN1d(ww,bb)\n",
    "dA = trap.forward(xx)\n",
    "print(dA)\n",
    "dW, dB, dX = trap.backward(xx,ww,dA)\n",
    "dW, dB, dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6859898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# permutation\n",
    "def _padding(X, n):\n",
    "    return np.pad(X, [(0,0), (n,n), (n,n)])\n",
    "\n",
    "# For back propagation\n",
    "def _padding_rm(X, n):\n",
    "    return X[:, n:-n, n:-n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b581c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(30).reshape(1,6,5)\n",
    "n = 3\n",
    "print(X)\n",
    "X_ = _padding(X, n)\n",
    "print(X_)\n",
    "X_ = _padding_rm(X_, n)\n",
    "print(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd56acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    Iterator to retrieve the mini-batch\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray of the following form, shape (n_samples, n_features)\n",
    "      Training data\n",
    "    y : ndarray of the following form, shape (n_samples, 1)\n",
    "      The correct answer value\n",
    "    batch_size : int\n",
    "      batch size\n",
    "    seed : int\n",
    "      Seed of random number in NumPy\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int64)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a451ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions in the Conv1d class\n",
    "def _convolve(X, k, P, S):\n",
    "    \"\"\"\n",
    "    Cyclic operation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray of the following form, shape (N, C, L)\n",
    "      the matrix to be traversed\n",
    "    k : ndarray of the following form, shape (F, C, Lf)\n",
    "      the matrix to be traversed\n",
    "    \"\"\"\n",
    "    L = X.shape[-1]\n",
    "    Lf = k.shape[-1]\n",
    "    Lo = self._output_size(L, Lf, P, S)\n",
    "    pad = int(L - Lf) \n",
    "    k_pad = np.pad(k, [(0,0), (0,0), (0, pad)]) \n",
    "    X_flat = X.reshape(len(X), -1) \n",
    "    k_flat = k_pad.flatten() \n",
    "    first_col = np.r_[k_flat[0], np.zeros(S * (Lo-1))] \n",
    "    first_row = k_flat  \n",
    "    toep = linalg.toeplitz(first_col, first_row) \n",
    "    toep_ = toep[::S]\n",
    "    toep_reorder = np.vstack(np.split(toep_, len(k), axis=1)) \n",
    "    output = toep_reorder@X_flat.T \n",
    "\n",
    "    return output.T.reshape(len(X), len(k), Lo)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2651b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve(X, k, P, S):\n",
    "    L = X.shape[-1]\n",
    "    Lf = k.shape[-1]\n",
    "    Lo = int((L + 2*P - Lf) / S + 1)\n",
    "    pad = int(L - Lf) \n",
    "    k_pad = np.pad(k, [(0,0), (0,0), (0, pad)])\n",
    "    X_flat = X.reshape(len(X), -1) \n",
    "    k_flat = k_pad.flatten() \n",
    "    first_col = np.r_[k_flat[0], np.zeros(S*(Lo-1))] \n",
    "    first_row = k_flat \n",
    "    toep = linalg.toeplitz(first_col, first_row) \n",
    "    toep = toep[::S] \n",
    "    toep_reorder = np.vstack(np.split(toep, len(k), axis=1)) \n",
    "    output = toep_reorder@X_flat.T \n",
    "    \n",
    "    return output.T.reshape(len(X), len(k), Lo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9cb430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "from scipy import linalg\n",
    "X = np.arange(6).reshape(1,1,6) # N,C,L\n",
    "w = np.arange(2).reshape(1,1,2) # F,C,Lf\n",
    "S = 2\n",
    "P = 0\n",
    "print(X)\n",
    "print(w)\n",
    "print(S)\n",
    "print(P)\n",
    "\n",
    "Z = convolve(X, w, P, S)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c3b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Back propagation (S=2)\n",
    "dA = Z\n",
    "Z = X\n",
    "\n",
    "w_ = np.transpose(w, (1,0,2)) \n",
    "w_ = np.flip(w_) \n",
    "N, F, Lo = dA.shape\n",
    "dA_dil = np.zeros((N, F, S*Lo-S+1)) \n",
    "dA_dil[... ,::S] = dA\n",
    "\n",
    "pad = w.shape[-1] - 1\n",
    "dA_pad = np.pad(dA_dil, [(0,0), (0,0), (pad, pad)]) \n",
    "dZ_ = convolve(dA_pad, w_, 0, 1) \n",
    "Z_ = np.transpose(Z, (1,0,2))  \n",
    "dA_ = np.transpose(dA_dil, (1,0,2))  \n",
    "dw_ = np.transpose(convolve(Z_, dA_, 0, 1), (1,0,2)) \n",
    "print(dw_)\n",
    "print(dZ_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5a3d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def update_dw(self, layer, grad):\n",
    "        return grad / layer.input.shape[0]\n",
    "    \n",
    "    def update_db(self, layer, grad):\n",
    "        return grad / layer.input.shape[0]\n",
    "        \n",
    "\n",
    "class AdaGrad:\n",
    "    def __init__(self):\n",
    "        self.Hw = 1e-8\n",
    "        self.Hb = 1e-8\n",
    "        \n",
    "    def update_dw(self, layer, grad):\n",
    "        self.Hw += (grad/layer.input.shape[0])**2\n",
    "        grad *= (1/np.sqrt(self.Hw)) / layer.input.shape[0]\n",
    "        \n",
    "        return grad\n",
    "    \n",
    "    def update_db(self, layer, grad):\n",
    "        self.Hb += (grad/layer.input.shape[0])**2\n",
    "        grad *= (1/np.sqrt(self.Hb)) / layer.input.shape[0]\n",
    "        \n",
    "        return grad\n",
    "        \n",
    "        \n",
    "class Conv1d:\n",
    "    def __init__(self, pad='SAME', stride=1):\n",
    "        self.pad = pad\n",
    "        self.stride = stride\n",
    "        self.A_ = None\n",
    "        self.dZ_ = None\n",
    "        self.dw_ = None\n",
    "        self.db_ = None\n",
    "        \n",
    "    def forward(self, Z, w, b):\n",
    "        # Define various shapes and parameters\n",
    "        L = Z.shape[-1]\n",
    "        Lf = w.shape[-1]\n",
    "        S = self.stride\n",
    "        \n",
    "        # Padding process\n",
    "        if self.pad == 'SAME':\n",
    "            if L % S == 0:\n",
    "                P = int((Lf - S) / 2)\n",
    "            else:\n",
    "                P = int((Lf - L%S) / 2)\n",
    "            Z = self._padding(Z, P)\n",
    "        elif self.pad == 'VALID':\n",
    "            P = 0\n",
    "        self.P = P\n",
    "        \n",
    "        self.A_ = self._convolve(Z, w, P, S) + b.reshape(1, len(b), 1)  # A(N,F,Lo)\n",
    "        \n",
    "        return self.A_\n",
    "    \n",
    "    def backward(self, Z, w, dA):\n",
    "        S = self.stride\n",
    "        \n",
    "        # dZ calculation\n",
    "        # transpose the matrix (leaving N and C) to perform the operation on each channel\n",
    "        w_ = np.transpose(w, (1,0,2)) # w(F,C,Lf) → w(C,F,Lf)\n",
    "        w_ = np.flip(w_) # flip up/down/left/right\n",
    "        # dilate dA\n",
    "        N, F, Lo = dA.shape\n",
    "        dA_dil = np.zeros((N, F, S*Lo-S+1)) # prepare dilation matrix\n",
    "        dA_dil[... ,::S] = dA\n",
    "        # padding dA_dil\n",
    "        pad = w.shape[-1] - 1\n",
    "        dA_pad = np.pad(dA_dil, [(0,0), (0,0), (pad, pad)]) # padding only L dimension\n",
    "        # Cyclic operation\n",
    "        dZ_ = self._convolve(dA_pad, w_, 0, 1) # dZ(N,C,L)\n",
    "        # Padding removal\n",
    "        if self.P == 0:\n",
    "            self.dZ_ = dZ_\n",
    "        else:\n",
    "            self.dZ_ = self._padding_rm(dZ_, self.P)\n",
    "        \n",
    "        # dw calculation            \n",
    "        # transpose the matrices to perform the operation on each filter (leaving F and C)\n",
    "        Z_ = np.transpose(Z, (1,0,2)) # Z(N,C,L) → Z(C,N,L)\n",
    "        dA_ = np.transpose(dA_dil, (1,0,2)) # dA(N,F,Lo) → dA(F,N,Lo)\n",
    "        self.dw_ = np.transpose(self._convolve(Z_, dA_, self.P, 1), (1,0,2)) # dw(C,F,Lf) → dw(F,C,Lf)\n",
    "        \n",
    "        # db calculation \n",
    "        # sum over all dimensions except F\n",
    "        self.db_ = np.sum(dA, axis=(0,2)) # db(F)\n",
    "        \n",
    "        return self.dZ_, self.dw_, self.db_\n",
    "    \n",
    "    def _output_size(self, L, Lf, P, S):\n",
    "        Lo = (L + 2*P - Lf) / S + 1   \n",
    "        return int(Lo)\n",
    "    \n",
    "    def _padding(self, X, n):\n",
    "        # for forward propagation\n",
    "        return np.pad(X, [(0,0), (0,0), (n,n)])\n",
    "\n",
    "    def _padding_rm(self, X, n):\n",
    "        # for back propagation\n",
    "        return X[... , n:-n]\n",
    "    \n",
    "    def _convolve(self, X, k, P, S):\n",
    "        L = X.shape[-1]\n",
    "        Lf = k.shape[-1]\n",
    "        Lo = self._output_size(L, Lf, P, S) \n",
    "\n",
    "        # Generate w for Teplitz\n",
    "        pad = int(L - Lf) # adjust the length of k to X\n",
    "        k_pad = np.pad(k, [(0,0), (0,0), (0, pad)]) # k(F,C,Lf) → k(F,C,L)\n",
    "        # Dimensionality reduction\n",
    "        X_flat = X.reshape(len(X), -1) # X(N,C,L) → X(N,CL)\n",
    "        k_flat = k_pad.flatten() # k(F,C,L) → k(FCL)\n",
    "        # generate the Teplitz matrix\n",
    "        first_col = np.r_[k_flat[0], np.zeros(S * (Lo-1))] # first column of Teplitz\n",
    "        first_row = k_flat # first row of Teplitz  \n",
    "        toep = linalg.toeplitz(first_col, first_row) # toep(Lo,FLC)\n",
    "        # Get only the row corresponding to the stride\n",
    "        toep = toep[::S]\n",
    "        # Block the matrix for each filter and recombine vertically\n",
    "        toep_reorder = np.vstack(np.split(toep, len(k), axis=1)) #toep(FLo,LC)\n",
    "        # matrix operation\n",
    "        output = toep_reorder@X_flat.T  # out(FLo,N)\n",
    "\n",
    "        return output.T.reshape(len(X), len(k), Lo)  # out(N,F,Lo)\n",
    "    \n",
    "\n",
    "class Linear:\n",
    "    def __init__(self):\n",
    "        self.A_ = None\n",
    "        self.dZ_ = None\n",
    "        self.dw_ = None\n",
    "        self.db_ = None\n",
    "        \n",
    "    def forward(self, Z, w, b):\n",
    "        self.A_ = Z @ w + b\n",
    "        \n",
    "        return self.A_\n",
    "    \n",
    "    def backward(self, Z, w, dA):\n",
    "        self.dZ_ = dA @ w.T\n",
    "        self.dw_ = Z.T @ dA\n",
    "        self.db_ = np.sum(dA, axis=0)\n",
    "        \n",
    "        return self.dZ_, self.dw_, self.db_\n",
    "\n",
    "        \n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.Z_ = None\n",
    "        self.dA_ = None\n",
    "        \n",
    "    def forward(self, A):\n",
    "        self.Z_ = 1 / (1+np.exp(-A))\n",
    "        \n",
    "        return self.Z_\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        self.dA_ = dZ * ((1 - self.Z_) * self.Z_)\n",
    "    \n",
    "        return self.dA_\n",
    "        \n",
    "        \n",
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        self.Z_ = None\n",
    "        self.dA_ = None\n",
    "        \n",
    "    def forward(self, A):\n",
    "        self.Z_ = np.tanh(A)\n",
    "        \n",
    "        return self.Z_\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        self.dA_ = dZ * (1 - self.Z_**2)\n",
    "        \n",
    "        return self.dA_\n",
    "        \n",
    "        \n",
    "class ReLu:\n",
    "    def __init__(self):\n",
    "        self.Z_ = None\n",
    "        self.dA_ = None\n",
    "        \n",
    "    def forward(self, A):\n",
    "        self.Z_ = np.maximum(A, 0)\n",
    "        \n",
    "        return self.Z_\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        self.dA_ = dZ * np.where(self.Z_ > 0, 1, 0)\n",
    "        \n",
    "        return self.dA_\n",
    "    \n",
    "        \n",
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.Z_ = None\n",
    "        self.dA_ = None\n",
    "        \n",
    "    def forward(self, A):\n",
    "        # Subtraction of constants to prevent overflow\n",
    "        C = np.max(A)\n",
    "        self.Z_ = np.exp(A - C) / np.sum(np.exp(A - C), axis=1)[:, None]\n",
    "        \n",
    "        return self.Z_\n",
    "    \n",
    "    def backward(self, y):\n",
    "        self.dA_ = self.Z_ - y\n",
    "        \n",
    "        return self.dA_\n",
    "    \n",
    "    \n",
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        self.shape = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.shape = X.shape\n",
    "        return X.reshape(self.shape[0], np.prod(self.shape[1:]))  # X(N,C,L) → X(N,CL)\n",
    "    \n",
    "    def backward(self, dX):\n",
    "        return dX.reshape(self.shape)\n",
    "                    \n",
    "        \n",
    "class SimpleInitializer:\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, shape_self):\n",
    "        W = self.sigma * np.random.standard_normal(shape_self)\n",
    "        return W\n",
    "    \n",
    "    def B(self, shape_self):\n",
    "        B = np.random.randn(n_nodes_self)\n",
    "        return B\n",
    "    \n",
    "\n",
    "class XavierInitializer:\n",
    "    def __init__(self):\n",
    "        self.sigma = None\n",
    "    \n",
    "    def W(self, n_nodes_prev, shape_self):\n",
    "        self.sigma = 1 / np.sqrt(np.prod(n_nodes_prev))\n",
    "        \n",
    "        W = self.sigma * np.random.standard_normal(shape_self)  # Use standard_normal which can take a tuple as an argument\n",
    "        return W\n",
    "    \n",
    "    def B(self, shape_self):\n",
    "        B = np.random.standard_normal(shape_self)\n",
    "        return B\n",
    "    \n",
    "    \n",
    "class HeInitializer:\n",
    "    def __init__(self):\n",
    "        self.sigma = None\n",
    "        \n",
    "    def W(self, n_nodes_prev, shape_self):\n",
    "        self.sigma = np.sqrt(2/np.prod(n_nodes_prev))\n",
    "        \n",
    "        W = self.sigma * np.random.standard_normal(shape_self)  # Use standard_normal which can take a tuple as an argument\n",
    "        return W\n",
    "    \n",
    "    def B(self, shape_self):\n",
    "        B = np.random.standard_normal(shape_self)\n",
    "        return B\n",
    "    \n",
    "    \n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    Iterator to retrieve the mini-batch\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray of the following form, shape (n_samples, n_features)\n",
    "      Training data\n",
    "    y : ndarray of the following form, shape (n_samples, 1)\n",
    "      The correct answer value\n",
    "    batch_size : int\n",
    "      batch size\n",
    "    seed : int\n",
    "      Seed of random number in NumPy\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int64)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "    \n",
    "    \n",
    "class Layer:\n",
    "    def __init__(self, combination, activation, \n",
    "                 initializer=None, optimizer=None, n_nodes_prev=None, w_shape=None, b_shape=None):\n",
    "        self.comb = combination\n",
    "        self.activ = activation\n",
    "        self.initializer = initializer # Use the initializer method to initialize self.W and self.B\n",
    "        self.optimizer = optimizer\n",
    "        self.n_nodes_prev = n_nodes_prev\n",
    "        self.w_shape = w_shape\n",
    "        self.b_shape = b_shape\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.prev = None\n",
    "        self.next = None\n",
    "        \n",
    "        if self.initializer:\n",
    "            self.w = self.initializer.W(self.n_nodes_prev, self.w_shape)\n",
    "            self.b = self.initializer.B(self.b_shape)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        if self.comb:\n",
    "            A = self.comb.forward(X, self.w, self.b)\n",
    "            Z = self.activ.forward(A)\n",
    "\n",
    "        else:\n",
    "            Z = self.activ.forward(X)\n",
    "        \n",
    "        self.input = X\n",
    "        self.output = Z\n",
    "        \n",
    "        if self.next:\n",
    "            return self.next.forward(Z)\n",
    "        else:\n",
    "            return Z\n",
    "    \n",
    "    def backward(self, y, lr):\n",
    "        if self.comb:\n",
    "            dA = self.activ.backward(y)\n",
    "            dZ, dw, db = self.comb.backward(self.input, self.w, dA)\n",
    "            \n",
    "            # Parameter update\n",
    "            self.w -= lr * self.optimizer.update_dw(self, dw)\n",
    "            self.b -= lr * self.optimizer.update_db(self, db)\n",
    "            \n",
    "        else:\n",
    "            dZ = self.activ.backward(y)\n",
    "        \n",
    "        if self.prev:\n",
    "            self.prev.backward(dZ, lr)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "class Scratch1dCNNClassifier:\n",
    "    def __init__(self, layers, epoch=100, sigma=0.1, lr=0.01, batch_size=100, verbose=False, **kwargs):\n",
    "        self.layers = layers\n",
    "        self.epoch = epoch\n",
    "        self.lr = lr\n",
    "        self.sigma = sigma\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_train = []\n",
    "        self.loss_val = []\n",
    "\n",
    "    def _connect_layers(self, layers):\n",
    "        for i, layer in enumerate(layers): \n",
    "            \n",
    "            if i == 0:\n",
    "                layer.next = self.layers[i+1]\n",
    "                \n",
    "            elif layer == self.layers[-1]:\n",
    "                layer.prev = self.layers[i-1]\n",
    "                \n",
    "            else:\n",
    "                layer.next = self.layers[i+1]\n",
    "                layer.prev = self.layers[i-1]\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        # Create a layer instance\n",
    "        self._connect_layers(self.layers)    \n",
    "        \n",
    "        for i in range(self.epoch):\n",
    "            \n",
    "            get_mini_batch_t = GetMiniBatch(X, y, batch_size=self.batch_size, seed=i)\n",
    "            \n",
    "            times = []\n",
    "            start = time.time()\n",
    "            \n",
    "            # List of losses for each mini batch\n",
    "            loss_batch_t = []\n",
    "            \n",
    "            for X_mini, y_mini in get_mini_batch_t:\n",
    "\n",
    "                # Forward propagation\n",
    "                output = self.layers[0].forward(X_mini)\n",
    "                # Back propagation\n",
    "                self.layers[-1].backward(y_mini, self.lr)\n",
    "\n",
    "                loss_batch_t.append(self.cross_entropy(output, y_mini))\n",
    "            \n",
    "            # Store the average loss for each epoch in self\n",
    "            loss_train = np.mean(loss_batch_t)\n",
    "            self.loss_train.append(loss_train)\n",
    "            \n",
    "            \n",
    "            # Estimation of validation data\n",
    "            if hasattr(X_val, '__array__') and hasattr(y_val, '__array__'):\n",
    "                \n",
    "                batch_size_v = int(self.batch_size * len(X_val)/len(X))\n",
    "                get_mini_batch_v = GetMiniBatch(X_val, y_val, batch_size=batch_size_v)\n",
    "                loss_batch_v = []\n",
    "\n",
    "                for X_mini, y_mini in get_mini_batch_v:\n",
    "                    \n",
    "                    output = self.layers[0].forward(X_mini)\n",
    "                \n",
    "                    loss_batch_v.append(self.cross_entropy(output, y_mini))\n",
    "            \n",
    "                # Store the average loss for each epoch in self\n",
    "                loss_val = np.mean(loss_batch_v)\n",
    "                self.loss_val.append(loss_val)\n",
    "\n",
    "            end = time.time()\n",
    "            times.append(end-start)\n",
    "\n",
    "            # Output of learning progress\n",
    "            if self.verbose:\n",
    "                print(\"Epoch {}: Train Loss {:.4f}, Val Loss {:.4f}\".format(i+1, loss_train, loss_val),\n",
    "                      \" --{:.4f}sec\".format(np.mean(times)))            \n",
    "                   \n",
    "    def predict(self, X):\n",
    "        output = self.layers[0].forward(X)\n",
    "        \n",
    "        return np.argmax(output, axis=1)\n",
    "        \n",
    "    def cross_entropy(self, X, y):\n",
    "        return (-1/len(X)) * np.sum((y*np.log(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b530c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Code to download the dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# Smoothing data\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "# Standardise on X\n",
    "X_train = X_train.astype(np.float64)\n",
    "X_test = X_test.astype(np.float64)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "# ytoone-hot encode\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "# Converting features to 3D\n",
    "X_train = X_train[:, None, :]\n",
    "X_val = X_val[:, None, :]\n",
    "X_test = X_test[:, None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352b0f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(model, title='Scratch CNN Loss'):\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(np.arange(len(model.loss_train)), model.loss_train, label='train loss')\n",
    "    plt.plot(np.arange(len(model.loss_val)), model.loss_val, label='val loss')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac2a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting of hyperparameters w(F,C,Lf) b(F)\n",
    "W1 = np.array((4,1,6)); B1 = 4\n",
    "W2 = np.array((8,4,6)); B2 = 8\n",
    "W3 = np.array((16,8,3)); B3 = 16\n",
    "W4 = np.array((752,188)); B4 = 188\n",
    "W5 = np.array((188,10)); B5 = 10\n",
    "\n",
    "# 5-layer network\n",
    "layer_1 = Layer(Conv1d(pad='SAME', stride=4), ReLu(), HeInitializer(), AdaGrad(), 784, W1, B1)  # 1,784 to 4,196\n",
    "layer_2 = Layer(Conv1d(pad='SAME', stride=4), ReLu(), HeInitializer(), AdaGrad(), 4*196, W2, B2)  # 4,196 to 8,49\n",
    "layer_3 = Layer(Conv1d(pad='VALID', stride=1), ReLu(), HeInitializer(), AdaGrad(), 8*49, W3, B3)  # 8,49 to 16,47\n",
    "layer_4 = Layer(None, Flatten())  # 16,47 to 752,\n",
    "layer_5 = Layer(Linear(), Sigmoid(), XavierInitializer(), AdaGrad(), 752, W4, B4)  # 752, to 188,\n",
    "output = Layer(Linear(), Softmax(), XavierInitializer(), AdaGrad(), 188, W5, B5)  # 188, to 10,\n",
    "\n",
    "params = {'epoch': 10, \n",
    "          'lr': 0.01,\n",
    "          'batch_size': 200,\n",
    "          }\n",
    "\n",
    "cnn = Scratch1dCNNClassifier(layers=[layer_1, layer_2, layer_3, layer_4, layer_5, output],\n",
    "                             verbose=True, **params)\n",
    "\n",
    "cnn.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "pred = cnn.predict(X_test)\n",
    "\n",
    "print(\"\\nTest Accuracy: {}\".format(accuracy_score(y_test, pred)))\n",
    "\n",
    "plot_loss(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdf71cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check estimation results\n",
    "plt.figure(figsize=(8,8))\n",
    "for i in range(20):\n",
    "    pred = cnn.predict(X_test[i][None,...])\n",
    "    true = y_test[i]\n",
    "    plt.subplot(4, 5, i+1)\n",
    "    plt.imshow(X_test[i].reshape(28,28), 'gray')\n",
    "    plt.title('pred {} \\ntrue [{}]'.format(pred, true), y=-0.45)\n",
    "    plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
